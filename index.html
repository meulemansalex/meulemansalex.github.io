---
layout: default
---

<article class="page">

  <h1> About </h1>

  <div class="entry">
    <p>What excites me is uncovering the basic principles behind intelligent systems acting in the world. How can a system learn to perceive the relevant things out there, and act according to some internal goals? If you open the gory black box of an intelligent system, which mechanisms will you find that lie at the basis of all these complex perceptions and actions?
    I focus on the artificial side of intelligence, while drawing inspiration from natural intelligence. Lately I'm intrigued by the power of self-supervised sequence learning, and all the (un)expected capabilities arising from it, such as in-context learning, hidden world models, action hierarchies etc. 
    Using insights from optimal control, theoretical neuroscience and optimization, I'm trying to make sense of it all. </p>

    <p>More concretely, my research focuses on (i) encovering hidden world-models in transformer models, 
    (ii) making reinforcement learning more sample efficient by introducing model-based credit assignment techniques for learning policies 
    grounded in causality and (iii) unifying action, perception and learning by bridging optimal control with theories of learning in the brain, 
    leading to novel learning algorithms for deep and recurrent neural networks that alleviate important drawbacks of 
    current neural network training methods. </p>
    
    <p>My research was awarded several spotlight and oral presentations at the top machine learning conferences. I have ongoing collaborations with among others Jo√£o Sacramento (ETH Zurich), Johannes von Oswald (Google Research), Greg Wayne (Google DeepMind), Nathaniel Daw (Princeton), Rafal Bogacz (Oxford) and Angelika Steger (ETH Zurich) on projects involving in-context learning in transformers, model-based credit assignment in reinforcement learning and methods for bayesian learning in the brain. 
    To get hands-on experience in training sota transformer models, I engaged in an internship at Microsoft Research Cambridge, focusing on combining large language models with knowledge bases, and on compressing and distilling large language models. 
    My research interests span a wide range, including in-context learning, optimal control, theoretical neuroscience, sequence learning, and trustworthy AI. </p>

    <p>Besides my love for science, I'm a passionate
    musician and on the weekends, you can find me either skiing or hiking in the beautiful Swiss Alps, or experimenting with homemade pizza. </p>

	<h3> Contact me </h3>
	<a href="mailto:ameulema@ethz.ch">ameulema@ethz.ch</a>
  </div>
</article>
