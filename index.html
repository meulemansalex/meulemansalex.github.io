---
layout: default
---

<article class="page">

  <h1> About </h1>

  <div class="entry">
    At the interface of machine learning and theoretical neuroscience, my research focuses on (i) encovering hidden world-models in transformer models, 
    (ii) making reinforcement learning more sample efficient by introducing model-based credit assignment techniques for learning policies 
    grounded in causality and (iii) creating new theories for learning in the brain by bridging optimal control with learning, 
    leading to novel learning algorithms for deep and recurrent neural networks that alleviate important drawbacks of 
    current neural network training methods. 
    My research was awarded several spotlight and oral presentations at the top machine learning conferences, and I am actively collaborating with Greg Wayne (Google DeepMind) and Nathaniel Daw (Princeton) on model-based credit assignment in reinforcement learning
    . To get hands-on experience in training sota transformer models, I engaged in an internship at Microsoft Research Cambridge, focusing on combining large language models with knowledge bases, and on compressing and distilling large language models. 
    My research interests span a wide range, including in-context learning, reinforcement learning, optimal control, causality, knowledge bases, deep learning, interpretability, theoretical neuroscience, sequence learning, bilevel optimization, continual learning, meta learning, self-supervised learning and trustworthy AI. 
      Besides my love for science, I'm a passionate
      musician and on the weekends, you can find me either skiing or hiking in the beautiful Swiss Alps, or experimenting with homemade pizza.

	<h3> Contact me </h3>
	<a href="mailto:ameulema@ethz.ch">ameulema@ethz.ch</a>
  </div>
</article>
