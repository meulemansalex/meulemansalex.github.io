---
layout: default
---

<article class="page">

  <h1> About </h1>

  <div class="entry">
    What excites me is uncovering the basic principles behind intelligent systems acting in the world. How can a system learn to perceive the relevant things out there, and act according to some internal goals? If you open the gory black box of an intelligent system, which mechanisms does it leverage to do all these complex perceptions and actions?
    I focus on the artificial side of intelligence, while drawing inspiration from natural intelligence. Lately I'm intrigued by the power of self-supervised sequence learning, and all the (un)expected capabilities arising from it, such as in-context learning, hidden world models, action hierarchies etc. 
    Using insights from optimal control, theoretical neuroscience and optimization, I'm trying to make sense of it all. 
    <br>
    More concretely, my research focuses on (i) encovering hidden world-models in transformer models, 
    (ii) making reinforcement learning more sample efficient by introducing model-based credit assignment techniques for learning policies 
    grounded in causality and (iii) unifying action, perception and learning by bridging optimal control with theories of learning in the brain, 
    leading to novel learning algorithms for deep and recurrent neural networks that alleviate important drawbacks of 
    current neural network training methods. 
    <br>
    My research was awarded several spotlight and oral presentations at the top machine learning conferences, and I am actively collaborating with Greg Wayne (Google DeepMind) and Nathaniel Daw (Princeton) on model-based credit assignment in reinforcement learning. 
    To get hands-on experience in training sota transformer models, I engaged in an internship at Microsoft Research Cambridge, focusing on combining large language models with knowledge bases, and on compressing and distilling large language models. 
    My research interests span a wide range, including in-context learning, reinforcement learning, optimal control, causality, knowledge bases, deep learning, interpretability, theoretical neuroscience, sequence learning, bilevel optimization, continual learning, meta learning, self-supervised learning and trustworthy AI. 
      Besides my love for science, I'm a passionate
      musician and on the weekends, you can find me either skiing or hiking in the beautiful Swiss Alps, or experimenting with homemade pizza.

	<h3> Contact me </h3>
	<a href="mailto:ameulema@ethz.ch">ameulema@ethz.ch</a>
  </div>
</article>
